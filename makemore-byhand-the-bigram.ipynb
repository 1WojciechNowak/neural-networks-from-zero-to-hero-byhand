{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef5dab7-8cd2-4dc8-bd2c-a25378e45d85",
   "metadata": {},
   "source": [
    "# Makemore - the bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9d65c-9ebc-49af-bf21-2aa9f41bce72",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519cb7e-e990-4a7c-9c2d-5c65244f40b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c22ec5-c8c2-4f5d-9afd-27033269c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ec1cd-c6ce-472d-bdc3-37aa840f1535",
   "metadata": {},
   "source": [
    "## Data discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b430f5-93ad-431e-a197-c5a7bfc08b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b27944-29e8-4b10-ab3c-c1c569b96593",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395485b-a3ed-4d9d-8cb8-5deb0ef456c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f98797-9e50-4487-a8e8-2c2a920bcfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(w) for w in words), max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f22050-7fb6-4ef6-b8d2-72784e580171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character level language model is predict predicting next character in the sequence \n",
    "# given already some concrete sequence of characters before it\n",
    "\n",
    "# Every single words in dataset words is a few examples of an input.\n",
    "# Given the emma => we have .e, em, mm, ma, a.\n",
    "# We have have following information:\n",
    "# The words in likely to start with e, \n",
    "# then after e there will likely be m,\n",
    "# after m we might likely have another m,\n",
    "# after m => a, and at a then words is likely to end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f00609-5fab-4e2a-a219-030f57e92eeb",
   "metadata": {},
   "source": [
    "## Bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522e85f-8e83-414a-8c94-78102bbe7159",
   "metadata": {},
   "source": [
    "We are working with 2 characters at a time\n",
    "We are looking at the 1 character that we are given as an input\n",
    "and we are trying to predict the next/consecutive one\n",
    "\n",
    "It's very simple and weak language model, but it's a good place to start :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d51bd-c064-4af2-ad05-64c5badd2bfc",
   "metadata": {},
   "source": [
    "### Exploring the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dabbbe7-f784-4d70-b1b5-2e64e28942a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "btoc = {} # bigrams to count\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1,ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2) \n",
    "        btoc[bigram] = btoc.get(bigram, 0) + 1\n",
    "\n",
    "btoc = sorted(btoc.items(), key=lambda x: x[1], reverse=True) # sorting by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ff4e0-e1b1-4f89-9c14-c4989cc656bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "btoc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3eac2-9cf8-47f0-8fd0-71c22e2bab6a",
   "metadata": {},
   "source": [
    "Let's convert the dict into 2 dimensional array. <br>\n",
    "The Rows -> 1st Char, The Columns -> 2nd Char, of the bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8877fa-d5c1-40b0-b45c-5d879ae840f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26 alphabet letters & one special character - '.' => It means we need 27x27 array to store the bigrams\n",
    "N = torch.zeros((27,27), dtype=torch.int32) # Input char index - to - Output char index -> CONTAINER for counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786056c-aa60-42d8-b894-ee4bb076b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words) + '.'))) # set of all lower case chars from words + special character '.'\n",
    "\n",
    "ctoi = {c:i for i,c in enumerate(chars)} # character to index\n",
    "itoc = {i:c for c,i in ctoi.items()} # index to character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ffeb7-0072-4a2e-8434-7d1701b18d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1,ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = ctoi[ch1]\n",
    "        ix2 = ctoi[ch2]\n",
    "        N[ix1, ix2] +=1 # Input char index - to - Output char index - TO -  count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f388d-5649-4bdc-ab53-b1f8a952995a",
   "metadata": {},
   "source": [
    "### Visualising the bigram tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccba477-ee10-4d39-a22f-b33ad1bbb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range (27):\n",
    "    for j in range(27):\n",
    "        chstr = itoc[i] + itoc[j]\n",
    "        plt.text(j,i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j,i, N[i,j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628892b-bc72-4b19-abed-a2b5e4c28ede",
   "metadata": {},
   "source": [
    "## Sampling from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0d538-78aa-4d53-8ccd-74aa6be420c8",
   "metadata": {},
   "source": [
    "#### Method: How to sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bbe1bd-a518-448c-ba28-902fc3d274d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # Will be used to generate an tensor of object in DETERMINISTIC/REPRODUCTIBLE way\n",
    "\n",
    "p = torch.rand(3, generator=g)\n",
    "p = p / p.sum() # normalizing to probability (sum up to 1.)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d063744-49d8-4805-953b-86dd42e00d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.multinomial.html\n",
    "# using multinomial to generate samples based on probability (the higher number of samples, the higher accuracy.\n",
    "num_samples = 10\n",
    "ix = torch.multinomial(p, num_samples=num_samples, replacement=True, generator=g)\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851c790-b364-4db3-9ea2-d6dc2ec2115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.bincount(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d7364-99fe-4979-b202-55a873cd18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "ix = torch.multinomial(p, num_samples=num_samples, replacement=True, generator=g)\n",
    "torch.bincount(ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fbeb29-a14b-4498-bfca-92805731039e",
   "metadata": {},
   "source": [
    "## Name generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43ad4b-54d3-4385-9be9-98f71014e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/notes/broadcasting.html\n",
    "\n",
    "# --- Optimization: Calculating the probabilities outside of the loop ---\n",
    "P = N.float() \n",
    "P /= P.sum(1, keepdim=True); # normalizing the N to Probabilties # arg=1, makes the sum column-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde4961-72fa-457c-ad36-f8eb4833f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10): # generation of n names; n = 10\n",
    "    out = [] # container for the chars that will form the NAME\n",
    "    ix = 0 # starting sampling with the '.'\n",
    "    while True:\n",
    "        # --- The below can be optimized (^ Look at the cell above) --- \n",
    "        # p = N[ix].float() # get output_chars counts for ix ['.' - in the first run] \n",
    "        # p = p / p.sum() # calculating probability\n",
    "        # --- Using the optimization ---\n",
    "        p = P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() # get one sample based on probability distribution\n",
    "        out.append(itoc[ix]) # index of the sample to char\n",
    "        if ix == 0: # if generated '.' -> the name has been generated\n",
    "            break;\n",
    "            \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6996c8bd-ba3c-4ece-8e79-3b0ef3a1efae",
   "metadata": {},
   "source": [
    "### Checking the model\n",
    "Sampling from untrained model (Equal probability for all chars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c86b1-f76d-48b5-99f0-8d3e4aae5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): # generation of n names; n = 10\n",
    "    out = [] # container for the chars that will form the NAME\n",
    "    ix = 0 # starting sampling with the '.'\n",
    "    while True:\n",
    "        p = torch.ones(27) / 27\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() # get one sample based on probability distribution\n",
    "        out.append(itoc[ix]) # index of the sample to char\n",
    "        if ix == 0: # if generated '.' -> the name has been generated\n",
    "            break;\n",
    "            \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd691df-e0ac-4199-9ae2-601990710a5f",
   "metadata": {},
   "source": [
    "### Result Summary\n",
    "Generated names look wrong. However, given the simplicty and weakness of the model, It is expected outcome. \n",
    "Model performs better then untrained one, which generates even worse \"names\".\n",
    "\n",
    "If one would like improve the accuracy. It is adviced to use more advanced mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee235b39-231a-4de7-848c-043a2cc42954",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af9d0c-1d67-437a-818d-2df649c0bf11",
   "metadata": {},
   "source": [
    "#### Bit of theory\n",
    "How to convert the probabilities into single number that measures the quality of the model? <br>\n",
    "\n",
    "The way to do it is a **Likelihood**! The likelihood is the product of all the probabilities. In good model the product should be as high as possible. <br>Given the values of prob, the product of them will be very small number. So for the convenience, it is adviced to use log_likelihood\n",
    ">When a, b & c are probs, the **log(a * b * c) = log(a) + log(b) + log(c)**\n",
    "\n",
    "\n",
    "The highest value of log_likelihood is 0. It would happen If all the probs are 1. Intuitively, the error should be minimized. Therefore, the the log_likelihood is then multiplied by -1, becoming **Negative Log Likelihood** (nnl). Then the NLL needs to be averaged - to become SINGLE metric and can further act as  **loss function**.\n",
    "<br>\n",
    "\n",
    "**The goal of the training is to find a params that minimize the loss function (normalized nll)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735149d-15e9-4a78-8817-7c1dcf387455",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0 # counter for normalization\n",
    "for w in words: # ['wojciech'] # to check probability of the name\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = ctoi[ch1]\n",
    "        ix2 = ctoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob) # (l\n",
    "        log_likelihood += logprob\n",
    "        n +=1\n",
    "        #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e17be-00db-492b-ace6-6082f3f74dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{log_likelihood=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8946d7e-3dd5-43d2-a62d-8d31600cf50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = -log_likelihood\n",
    "print(f'{nll=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c1f61-ed39-4de6-a797-53d4c64c43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nll / n # normalized (by count - making an average); called normalized nnl or averaged nnl\n",
    "print(f'{loss=}') # might work as a good loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278d4eb-9d8f-4e7b-ac1a-8298cb10d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOAL: Maximize likelihood of the data w.r.t. model parameters (statistical modeling)<br>\n",
    "#equivalent to maximizing the log_likelihood (because log is monotonic)<br>\n",
    "#equivalent to minimizing the negative log_likelihood<br>\n",
    "#equivalent to minimizing the average negative log_likelihood<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567617b4-bd5c-412a-9c26-615240ddce47",
   "metadata": {},
   "source": [
    "### Model smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba187107-2ffe-4ac9-af15-46b28914bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0 # counter for normalization\n",
    "for w in ['wojciechx']: # to check probability of the name\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = ctoi[ch1]\n",
    "        ix2 = ctoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob) # (l\n",
    "        log_likelihood += logprob\n",
    "        n +=1\n",
    "        print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5808c3-fb6f-47b0-9b4d-f101ce6282ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{log_likelihood=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973124e5-1731-4406-9498-079cd2ca0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = -log_likelihood\n",
    "print(f'{nll=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b068c-cab3-4b0a-ac80-c575f1a1cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nll / n # normalized (by count - making an average); called normalized nnl or averaged nnl\n",
    "print(f'{loss=}') # might work as a good loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4901d26-eef4-480e-aab5-c3bc7b02f040",
   "metadata": {},
   "source": [
    "### Theory\n",
    "For 'wojciechx', there is no probability to generate the name. It is because the 'hx' has 0 probability. <br>\n",
    "However, the 'wojciechx' has some probability to occur. For that kind of cases the thing called 'model smoothing' has been introduced.\n",
    "\n",
    "The goal is to add the number to the ins-to-outs count to add some very small probability for the combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c2f4c-27d1-4e4a-872f-5b0be4c5d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+5).float() # Add some value to N, make sure that it won't be 0 probabilities\n",
    "P /= P.sum(1, keepdim=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7065055-7d65-4c58-9cda-0f6edce02554",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0 # counter for normalization\n",
    "for w in ['wojciechx']: # to check probability of the name\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = ctoi[ch1]\n",
    "        ix2 = ctoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob) # (l\n",
    "        log_likelihood += logprob\n",
    "        n +=1\n",
    "        print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d1139-621f-4f8f-b9c9-6bd3b8097cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{log_likelihood=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71329c02-f9b9-46be-ab13-6dfb3726645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = -log_likelihood\n",
    "print(f'{nll=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24af618-10f6-45c7-b90a-3fd35151ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nll / n\n",
    "print(f'{loss=}') # Model smoothing introduced some probability to 'hx' -> The chances for \"Wojciechx\" has increased,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
