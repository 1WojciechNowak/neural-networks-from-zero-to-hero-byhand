{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a61edec-f0aa-43de-996e-092260c88788",
   "metadata": {},
   "source": [
    "# Makemore by hand - building the mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1233c56-5a01-487c-b127-9cd146e40d39",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60e248c-a3b2-4ce0-9bb4-802b89994e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/wojciechnowak/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0723ce15-5b7b-41c3-ba39-abb6d6aeef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7634b9-cfbd-4e7d-a5f7-2b8ff5e4c92e",
   "metadata": {},
   "source": [
    "## Data discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c3b404-9b2e-485c-9d4f-de305b6af6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe93b94-5ffb-453e-9521-82fdaa0cd892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66fad5f-1c72-48f0-859d-9434b56a929a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00054462-10c2-40cf-b6d3-7761bc96e1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(w) for w in words), max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732a10d1-72f3-48c0-a0f0-2b9ddfeb50d0",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a402ac73-0d52-4115-857f-a7278bd72295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character level language model is predict predicting next character in the sequence,\n",
    "# given already some concrete sequence of characters before it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b352326-57ef-4e4c-a96a-e101cd203f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP - Following Bengio et al. 2003\n",
    "\n",
    "# Applying the approach from Bengio et al. 2003 into the character level language model\n",
    "# We would take 3 previous chars are we would try to predict 4th char in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906d849-00bc-4466-af32-2c794f7d68aa",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41beeaa9-f5c6-4de0-82b9-2bbdf91837c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words) + '.'))) # set of all lower case chars from words + special character '.'\n",
    "\n",
    "ctoi = {c:i for i,c in enumerate(chars)} # character to index\n",
    "itoc = {i:c for c,i in ctoi.items()} # index to character\n",
    "\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf197c4f-3793-4899-b8df-437fffecf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for dataset creation\n",
    "def make_dataset(input, size = None, verbose = False):\n",
    "    block_size = 3 # context length: how many chars do the model take to predict the next one\n",
    "    X, Y = [], []\n",
    "\n",
    "    words = input\n",
    "    if size is not None:\n",
    "        words = words[:size] # Let's take first n words\n",
    "\n",
    "    for w in words:\n",
    "        if verbose is True:\n",
    "            print(w)\n",
    "        context = [0] * block_size\n",
    "\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch in w + '.':\n",
    "            ix = ctoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            if verbose is True:\n",
    "                print(''.join(itoc[i] for i in context), '--->', itoc[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a24534e9-caae-4197-b582-ebeaaded0314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n"
     ]
    }
   ],
   "source": [
    "# Let's create exploratory dataset (3 first words only!\n",
    "X, Y = make_dataset(words, 3, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dd7f263-f45b-47c1-aa2c-c9e28c868aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3]), torch.int64, torch.Size([16]), torch.int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype # 16 sample inputs, each of size 3\n",
    "\n",
    "# 3 first words made 16 samples, each of size 3 chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36387f7e-d928-4058-bbf7-c8cf50c26c4f",
   "metadata": {},
   "source": [
    "## Theory & Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869af346-f2c4-473c-ab91-52be5b392b57",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc6445fe-4d6a-4d0f-a9c9-ab4f7837d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27,2)) # Embeddings generation: Each of the characters would have representation in 2 dimensional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef5e6f-61ae-4651-be9a-324b93312ce9",
   "metadata": {},
   "source": [
    "#### Playground: Single Integer Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efdf8e8e-6a59-4dad-b73e-031919a72ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4608, -1.1097])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st way!\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "040d78cc-8ef2-4b56-8fb3-bd239a638f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4608, -1.1097])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd way!\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8df42df-f6ef-46b8-96ea-5131a68f80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the 2 ways of embedding\n",
    "# -> It can be either considered as a 1st layer of neural network (C as w Weight matrix, no bias, no non-linearity)\n",
    "# -> or as an Integer indexing to lookup table C\n",
    "\n",
    "# We would use Indexing, since it is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df1938a8-96b7-46b6-934c-f30f28ae2d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4608, -1.1097])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the 5th row\n",
    "\n",
    "C[5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46c2a5d2-24f7-464b-a601-6803b0f1d398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4608, -1.1097],\n",
       "        [ 2.2593,  0.0899],\n",
       "        [-0.4833,  0.9610],\n",
       "        [-0.4833,  0.9610],\n",
       "        [-0.4833,  0.9610]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's possible to get an array of arbitrary rows\n",
    "# While indexing through multi dim matrix, the 1st is row, the 2nd is column\n",
    "\n",
    "C[[5,8,9,9,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "762a7741-ba7e-407f-8e2f-f635aa766c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4608, -1.1097],\n",
       "        [ 2.2593,  0.0899],\n",
       "        [-0.4833,  0.9610],\n",
       "        [-0.4833,  0.9610],\n",
       "        [-0.4833,  0.9610]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing with tensor of ints\n",
    "# Getting the embeddings of (6th, 9th,... character)\n",
    "\n",
    "C[torch.tensor([5,8,9,9,9])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91d99abd-37f2-482a-9480-9c8a6323123d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7762, -0.1100],\n",
       "         [ 1.7762, -0.1100],\n",
       "         [ 1.7762, -0.1100]],\n",
       "\n",
       "        [[ 1.7762, -0.1100],\n",
       "         [ 1.7762, -0.1100],\n",
       "         [-0.4608, -1.1097]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing with multidimentional tensor of Ints, for the first 2 records\n",
    "# Gettubg the embeddings of the ... & ..e (in the word emma)\n",
    "\n",
    "C[X][:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c104277b-ae24-4c71-aca7-4b8efcb42bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 3 words made 16 inputs, each input contains 3 chars, each char has 2 dim embedding\n",
    "\n",
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a037e21d-465c-43f6-a299-f5bac3d2fd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.7762, -0.1100],\n",
       "         [ 1.7762, -0.1100],\n",
       "         [ 1.7762, -0.1100]]),\n",
       " tensor([[ 1.7762, -0.1100],\n",
       "         [ 1.7762, -0.1100],\n",
       "         [-0.4608, -1.1097]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I.e. embeddings of the first 2 inputs\n",
    "\n",
    "C[X][0], C[X][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72a282c8-7a12-4aaa-a6ef-ce26c73b33d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd input, 3rd char\n",
    "# embedding of the char e\n",
    "X[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fd8ca03-f184-410b-a56e-f387a7d033a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Char e is indexed as 5 in itoc\n",
    "itoc[X[1,2].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "268e4bf2-77c1-4759-8625-eb0ae7d81b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4608, -1.1097])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding of letter E\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1db811c4-d7ff-4236-936b-748881ab4f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4608, -1.1097])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding of the 3rd letter in 2nd input - E\n",
    "C[X][1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a48e79b-05b4-4f21-86aa-b426200013fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the embeddings are randomly generated and have no meaning at all! \n",
    "# In order to make them meaningfull, the MLP needs to be trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae99414-597a-41c8-8e0d-881987198df3",
   "metadata": {},
   "source": [
    "#### The input - X - embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29bccd50-6cf0-4f3c-8fce-3cfcfec80fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7762, -0.1100],\n",
       "         [ 1.7762, -0.1100],\n",
       "         [ 1.7762, -0.1100]],\n",
       "\n",
       "        [[ 1.7762, -0.1100],\n",
       "         [ 1.7762, -0.1100],\n",
       "         [-0.4608, -1.1097]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb[:2] # first 2 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebcee485-0c4d-4473-8b32-8b84c025b230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 3 words made 16 samples, each of size 3 chars, each of them has 2 dimensional embeddings\n",
    "emb.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9145d561-1f45-48b5-8cd1-d8850a9cc2b7",
   "metadata": {},
   "source": [
    "### Hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46bfccbe-a583-480f-95b6-da59608e5975",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 100\n",
    "W1 = torch.randn((6,n_neurons)) # 6 inputs 3 letters, each is represented by 2 dim embedding\n",
    "b1 = torch.randn(n_neurons) # each bias initialized randomly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5525729d-b2a8-48b1-aaf3-32cd1c8ca7c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (48x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m \u001b[38;5;241m+\u001b[39m b1 \u001b[38;5;66;03m# This is intended to fail :)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# the embs - [16x3x2] cannot be multiplied with [6x100]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# It needs to be transformed to have 2nd dimension 6 \u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (48x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "emb @ W1 + b1 # This is intended to fail :)\n",
    "\n",
    "# the embs - [16x3x2] cannot be multiplied with [6x100]\n",
    "# It needs to be transformed to have 2nd dimension 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50792b-a18f-4ed4-85c4-7ee651820453",
   "metadata": {},
   "source": [
    "#### The dimension change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b35d1f0-7912-4d89-8042-c3245c9a32c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100],\n",
       "        [-0.4608, -1.1097],\n",
       "        [ 2.0448,  0.5986],\n",
       "        [ 1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100],\n",
       "        [ 1.2801,  0.0483],\n",
       "        [ 0.6551,  2.8346],\n",
       "        [-0.4833,  0.9610],\n",
       "        [-1.8117,  0.3492],\n",
       "        [ 1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100],\n",
       "        [-0.0487,  1.3848]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st way\n",
    "emb[:,0,:] # all 16 embeddings (2 dim each) of the first word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe79cd7b-8cd7-45e6-8df8-75b423b1d944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f48929b1-eb7a-4dd2-b5b7-0f36aa97cde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7762, -0.1100,  1.7762, -0.1100,  1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100, -0.4608, -1.1097],\n",
       "        [ 1.7762, -0.1100, -0.4608, -1.1097,  2.0448,  0.5986],\n",
       "        [-0.4608, -1.1097,  2.0448,  0.5986,  2.0448,  0.5986],\n",
       "        [ 2.0448,  0.5986,  2.0448,  0.5986, -0.0487,  1.3848],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100,  1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100,  1.2801,  0.0483],\n",
       "        [ 1.7762, -0.1100,  1.2801,  0.0483,  0.6551,  2.8346],\n",
       "        [ 1.2801,  0.0483,  0.6551,  2.8346, -0.4833,  0.9610],\n",
       "        [ 0.6551,  2.8346, -0.4833,  0.9610, -1.8117,  0.3492],\n",
       "        [-0.4833,  0.9610, -1.8117,  0.3492, -0.4833,  0.9610],\n",
       "        [-1.8117,  0.3492, -0.4833,  0.9610, -0.0487,  1.3848],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100,  1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100, -0.0487,  1.3848],\n",
       "        [ 1.7762, -0.1100, -0.0487,  1.3848, -1.8117,  0.3492],\n",
       "        [-0.0487,  1.3848, -1.8117,  0.3492, -0.0487,  1.3848]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using torch cat to squash the emb 16x3x2 to 16x6\n",
    "# So inputs [1, 2] [2,3] [3,4] being concatenated will make [1,2,2,3,3,4]\n",
    "\n",
    "torch.cat([emb[:,0,:], emb[:,1,:], emb[:,2,:]], dim=1) # concat along the dim 1 column-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc28d52e-944e-4c95-b645-bf13b8453f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7762, -0.1100,  1.7762, -0.1100,  1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100, -0.4608, -1.1097],\n",
       "        [ 1.7762, -0.1100, -0.4608, -1.1097,  2.0448,  0.5986],\n",
       "        [-0.4608, -1.1097,  2.0448,  0.5986,  2.0448,  0.5986],\n",
       "        [ 2.0448,  0.5986,  2.0448,  0.5986, -0.0487,  1.3848],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100,  1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100,  1.2801,  0.0483],\n",
       "        [ 1.7762, -0.1100,  1.2801,  0.0483,  0.6551,  2.8346],\n",
       "        [ 1.2801,  0.0483,  0.6551,  2.8346, -0.4833,  0.9610],\n",
       "        [ 0.6551,  2.8346, -0.4833,  0.9610, -1.8117,  0.3492],\n",
       "        [-0.4833,  0.9610, -1.8117,  0.3492, -0.4833,  0.9610],\n",
       "        [-1.8117,  0.3492, -0.4833,  0.9610, -0.0487,  1.3848],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100,  1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100, -0.0487,  1.3848],\n",
       "        [ 1.7762, -0.1100, -0.0487,  1.3848, -1.8117,  0.3492],\n",
       "        [-0.0487,  1.3848, -1.8117,  0.3492, -0.0487,  1.3848]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd way\n",
    "torch.cat(torch.unbind(emb,1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2d216e3-4edb-43c1-a097-4d5599af6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st and 2nd way of changing the dimensionality are equivalent\n",
    "\n",
    "# But, both are not efficient!\n",
    "# These create a new variable in a memory.\n",
    "\n",
    "# There is 3rd - More efficient way to do it! With a VIEW function.\n",
    "# It created a logical view!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f71b90ec-83a2-495c-9ca6-8bf8acf834fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "torch.Size([18])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "# 3rd way\n",
    "\n",
    "# Efficient way to change the in # We can use View!\n",
    "\n",
    "# Let's test it\n",
    "a = torch.arange(18)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(a.view(3,6)) # Using View!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dada4cf1-0321-421d-bb17-df7123549208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7762, -0.1100,  1.7762, -0.1100,  1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100, -0.4608, -1.1097],\n",
       "        [ 1.7762, -0.1100, -0.4608, -1.1097,  2.0448,  0.5986],\n",
       "        [-0.4608, -1.1097,  2.0448,  0.5986,  2.0448,  0.5986],\n",
       "        [ 2.0448,  0.5986,  2.0448,  0.5986, -0.0487,  1.3848],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100,  1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100,  1.2801,  0.0483],\n",
       "        [ 1.7762, -0.1100,  1.2801,  0.0483,  0.6551,  2.8346],\n",
       "        [ 1.2801,  0.0483,  0.6551,  2.8346, -0.4833,  0.9610],\n",
       "        [ 0.6551,  2.8346, -0.4833,  0.9610, -1.8117,  0.3492],\n",
       "        [-0.4833,  0.9610, -1.8117,  0.3492, -0.4833,  0.9610],\n",
       "        [-1.8117,  0.3492, -0.4833,  0.9610, -0.0487,  1.3848],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100,  1.7762, -0.1100],\n",
       "        [ 1.7762, -0.1100,  1.7762, -0.1100, -0.0487,  1.3848],\n",
       "        [ 1.7762, -0.1100, -0.0487,  1.3848, -1.8117,  0.3492],\n",
       "        [-0.0487,  1.3848, -1.8117,  0.3492, -0.0487,  1.3848]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making view on the emb!\n",
    "emb.view(16,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b60587-73ec-4445-8030-8cfb34144699",
   "metadata": {},
   "source": [
    "#### Hidden Layer - Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a12e5aa3-3a79-4d14-bc63-eedf8268322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(emb.shape[0],6) @ W1 + b1) # or!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9dc376c0-9929-4b1f-9076-455835e9c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3dd9d6b1-6a52-49c8-985a-d69e9dc4ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h - hidden activation states # values betwee -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40b48980-2743-4ded-aa57-e7bb0ff68c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c55ca6-89f3-4c0d-b1f8-dd153cd1c063",
   "metadata": {},
   "source": [
    "### Output Layer & Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d53b5aa0-ef94-4f98-8a51-3cdd6980093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((n_neurons, 27)) # 100 Outputs from h layer and 27 output chars\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71910ece-ec56-49d7-96b9-df0329b47d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-12.5294)\n",
      "tensor(-6.1491)\n"
     ]
    }
   ],
   "source": [
    "logits = h @ W2 + b2 # logits - log-counts\n",
    "logits[0] # log counts for the 1st input (...)\n",
    "\n",
    "# logits - log-counts -> the smaller value, the less frequent occurance of the letter (after the training)!\n",
    "print(logits[0][16]) # very small after undoing log by exp\n",
    "print(logits[0][17]) # very big after undoing log by exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9760c50-bc2b-47fe-a0b3-6a9187193687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 27])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60df41a1-a757-4a2a-b2f2-94bbcb61b957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6185e-06)\n",
      "tensor(0.0021)\n"
     ]
    }
   ],
   "source": [
    "counts = logits.exp() # make the counts from log-counts withs exponential function\n",
    "print(counts[0][16]) # very small\n",
    "print(counts[0][17]) # very big!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ed23ec2-7545-4a8d-9b83-ded877d70749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2874e-06, 7.3614e-06, 6.6563e-01, 6.6257e-05, 4.7296e-02, 2.3623e-08,\n",
       "        1.1036e-04, 2.2232e-02, 2.2925e-07, 4.1375e-03, 1.5786e-05, 3.9938e-08,\n",
       "        2.0520e-02, 5.6062e-03, 7.8984e-03, 1.2472e-01, 2.3460e-11, 1.3844e-08,\n",
       "        4.1488e-12, 6.9159e-02, 1.9947e-03, 1.1957e-04, 1.5374e-04, 3.0275e-02,\n",
       "        1.5703e-07, 1.2550e-07, 5.2894e-05])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "prob[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071c05b-0452-4519-8f7b-b73863b4c456",
   "metadata": {},
   "source": [
    "#### Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc5d392d-8547-4165-8d33-d93d598fd144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.1946)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-prob[torch.arange(16), Y].log().mean() # loss function! - negative log likilyhood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85c72645-e2b4-4314-a0eb-46a06c3f8dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But, there is an issue with it: \n",
    "# Let's dive deep!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbf04398-3599-4889-9867-4d57c8ef11d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.0466e-04, 3.3281e-04, 6.6846e-03, 9.9208e-01])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([-2,-3,0,5]) # Good/Reasonable logits distribution!\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs # Probs have reasonable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e446dde-da5c-4c9d-b8e5-72669b52383e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 3.3311e-04, 6.6906e-03, 9.9298e-01])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([-100,-3,0,5]) # One of the logits is very small -100\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs # Prob of the '-100' is 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3eacfbd5-2dee-453e-9cf0-e299ebc45a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([-100,-3,0,100]) # One of the logits is very big!\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs # Prob of the 100 is NaN (is got out-of-range of the floating point number)\n",
    "\n",
    "# Big numbers cannot be passed to the expression\n",
    "\n",
    "# CONCLUSION - The approach presented in not stable numerically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad783b5d-4a31-46a6-8309-cab805da8572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 4.9787e-02, 1.0000e+00,        inf])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts # The big number becomes infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53b4ca38-b75f-42bf-a806-a8ffca83947a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the solution numerically stable:\n",
    "\n",
    "logits = torch.tensor([-100,-3,0,100]) - 100 # Offsetting the tensor values to prevent NaN values\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs # Prob of the 100 is NaN (is got out-of-range of the floating point number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8b262fe-24e9-4930-a273-6c8ccdac95a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70569228-08c6-434f-861f-a6d5e4577d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The negative numbers behave good\n",
    "# The high, positive numbers can make the exp overflow - getting NaN\n",
    "\n",
    "# We can (and pytorch does it internally) offset the logits with the highest value from the logits tensor\n",
    "# As a result, the probs are always well-behaved numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b079565d-f1d3-4759-b2eb-b4a95e7632fe",
   "metadata": {},
   "source": [
    "#### Intro of Cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "743b73b9-09ea-4576-81be-c6a6f208c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of offsetting the logits, the pytorch function - cross_entropy can be used, it:\n",
    "\n",
    "# 1. Memory optimiation, it will not allocate arrays as in the \"step by step\" calc from above\n",
    "# 2. Cross_entropy also can behave better numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c96c4814-f87a-4ef6-95f1-1b3ffa4a9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2 # logits - log-counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1db26a17-13ac-4d1f-9312-8f6587b92f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.1946)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Y) # Use cross entropy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8303a1a-6f93-4cc6-bfc5-3fb938655099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.1946)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss function! - negative log likilyhood! - manual calc - same as with cross_entropy\n",
    "-prob[torch.arange(16), Y].log().mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ea6dba2-5f60-47fb-914f-be5dae00a6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([-100,-3,0,100]) - 100 # To prevent nan (not well-hehaved..) pytorch offset the \n",
    "# The negative numbers behave good,\n",
    "# but very high numbers make the exp overflow - getting NaN\n",
    "# We can (and pytorch does it internally) offset the logits with the highest value from the logits tensor\n",
    "# As a result -> Thhe probs are always well-behaved numerically :)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc711a-2c92-413f-8455-fe2385d548cc",
   "metadata": {},
   "source": [
    "## The MLP optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2036484c-d7ee-4ae0-90b2-3c34751c3ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d8c8b97-d123-4703-a38c-f7123b988075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_init(n_neurons = 100):\n",
    "    g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "    C = torch.randn((27,2), generator=g)\n",
    "    W1 = torch.randn((6, n_neurons), generator=g)\n",
    "    b1 = torch.randn(n_neurons, generator=g)\n",
    "    W2 = torch.randn((n_neurons,27), generator=g)\n",
    "    b2 = torch.randn(27, generator=g)\n",
    "\n",
    "    params = [C, W1, b1, W2, b2]\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    return C, W1, b1, W2, b2, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cec16cc9-f561-4b2d-a9a8-27e2b72552b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C, W1, b1, W2, b2, parameters = nn_init(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "213d93cb-81e9-486f-b532-8dedfe976b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(C, W1, b1, W2, b2):\n",
    "    emb = C[X] # (16,3,2)\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) # (16,100)\n",
    "    logits = h @ W2 + b2 # (32,27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "146b1185-1b42-4c5c-8fa1-5273b62e6950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parameters in total\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48c9347f-5bea-41e1-9459-17a3adaab973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.241714477539062\n",
      "4.101349353790283\n",
      "1.0044928789138794\n",
      "0.6979317665100098\n",
      "0.39906036853790283\n",
      "0.3665124177932739\n",
      "0.3108588755130768\n",
      "0.2987688183784485\n",
      "0.27265694737434387\n",
      "0.26634684205055237\n"
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "training_iter = 30\n",
    "lr = 0.2\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # forward pass\n",
    "    emb = C[X] # (16,3,2)\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) # (16,100)\n",
    "    logits = h @ W2 + b2 # (16,27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    \n",
    "    if i % (training_iter / 10) == 0:\n",
    "        print(loss.item())\n",
    "        \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    lossi.append(calc_loss(C, W1, b1, W2, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38bb6456-901c-4c50-afc7-9afb014ccfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x125da6310>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e612608e-313d-4722-9ab8-244136dc2fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25256821513175964"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a score i.e. - 0.3 - it seems that network makes VERY GOOD predictions!\n",
    "# But it has been trained only on 16 inputs, generated from 3 words.\n",
    "\n",
    "calc_loss(C, W1, b1, W2, b2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe64d5c3-909c-484b-943e-39ff1096215d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([15.3655, 15.6087, 18.8534, 14.7799, 15.3627, 15.3655, 15.0088, 15.6660,\n",
       "        13.7246, 15.5152, 13.0374, 21.7494, 15.3655, 15.0663, 15.7742, 20.4630],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([15, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0, 15, 22,  1,  0]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The NN will not be able to react exactly 0\n",
    "\n",
    "logits.max(1)\n",
    "# Provides max values and its indices for the each input! \n",
    "# The indices are similar but not exact to Ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4b65fccd-605a-4f68-a8cd-600bc8ccce54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5792967d-fec0-4536-9e81-2c087b6def75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is why The NN is not be able to react exactly 0:\n",
    "\n",
    "# Given the input samples\n",
    "# (...) -> e (in emma)\n",
    "# (...) -> a (in olivia)\n",
    "# (...) -> s (in sophia)\n",
    "\n",
    "# There is no unique input for given output \n",
    "# that's only case (1 in -> 1 out) when the NN would ideally overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99252b2-8220-411c-aaf9-8b82edcd8e4e",
   "metadata": {},
   "source": [
    "## Training on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a0acbbd7-c3db-4291-a9f8-89cd86d4f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here to be done on the whole dataset :)\n",
    "\n",
    "# Let's create the dataset\n",
    "X, Y = make_dataset(words, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a3cb45d5-96ff-4f87-9cb8-4e71024a608d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7eeac4b7-aec7-43c5-9cb7-3a7239892dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "C, W1, b1, W2, b2, parameters = nn_init(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "066b0c19-5f9b-47bd-a47a-cd51a11c1128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7b739f76-1c73-455a-8921-96f86807f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2de95-7257-4117-aa64-145c1a720c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 # (32,27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    \n",
    "    # if i % (training_iter / 100) == 0:\n",
    "        #print(loss.item())\n",
    "        \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a068bbe-1b27-4998-9c66-4c80d27adb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change loss during the training\n",
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b3e8a-5f8f-4a7e-8166-de69aebf529e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
